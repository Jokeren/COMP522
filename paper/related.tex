\paragraph{Task pools.}
There is a large body of work on lock-free unbounded FIFO queues and LIFO
stacks~\cite{Gidenstam:2010:CLQ:1940234.1940266,Hendler:2004:SLS:1007912.1007944,
Hoffman:2007:BQ:1782394.1782423, Michael:1996:SFP:248052.248106,Moir:2005:UEI:1073970.1074013}.
The problem with such algorithms is that due to the inherent need for ordering all operations, they
generally have high contention and hence do not scale well being therefore less appealing for use as 
consumer-producer task pools. 

A number of previous works have recognized this limitation, and observed that strict FIFO
order is seldom needed in multi-core systems~\cite{Afek:2010:SPP:1885276.1885295,springerlink:10.1007/978-3-642-17653-1_29,
Basin:2011:CST:2075029.2075087,Sundell:2011:LAC:1989493.1989550}. However, all of these solutions
use strong atomic operations, at least in every consume operation. Most of
them~\cite{Afek:2010:SPP:1885276.1885295,springerlink:10.1007/978-3-642-17653-1_29,
Basin:2011:CST:2075029.2075087} do not partition the pool among chips, and therefore do not achieve
good locality and cache-friendliness, which limits their scalability on NUMA systems~\cite{Basin:Thesis:2011}.

The non-FIFO pool that is closest to our work is the Concurrent Bags of
Sundell et al.~\cite{Sundell:2011:LAC:1989493.1989550}, which, like SALSA, is composed of
per-producer chunk lists. Unlike our pool, however, their algorithm uses strong atomic operations
upon each consume. In addition, steals are performed in the granularity of single tasks and
not whole chunks as in SALSA. Overall, Concurrent Bags throughput does not scale linearly with the number of participating threads.

\paragraph{Techniques.}
Variations of techniques we employ were previously used in various contexts. 
Work-stealing~\cite{Blumofe:1999:SMC:324133.324234} is a standard way to reduce
contention by using individual per-consumer pools, where tasks may be stolen from one pool to
another. 
Partitioning of tasks into per-consumer pools also allows optimizing performance in the
case where a consumer is working on its own pool without being interrupted by stealing; we refer to
this case as the \emph{fast-path}. The concept of a synchronization-free fast-path previously
appeared in works on scheduling queues,
e.g.,~\cite{Arora:1998:TSM:277651.277678,Hendler:2006:DNW:1160290.1160294}. However, these works
assume that the same process is both the producer and the consumer, and hence the
synchronization-free fast-path is actually used only when a process transfers data to \emph{itself}.
On the other hand, our pool is synchronization-free even when tasks are transfered among multiple
threads; our synchronization-free fast-path is used also when multiple producers produce data for
a single consumer. We do not know of any other work that supports synchronization-free data
transfer among different threads.

We improve the efficiency of stealing by transferring a chunk of tasks upon every steal
operation. Hendler et al.~\cite{Hendler:2002:NSW:571825.571876} have proposed stealing of multiple
items by copying a range of tasks from one dequeue to another. Unfortunately, this approach requires
costly CAS operations on the fast-path and introduces non-negligible overhead for item copying. In
contrast, our approach of chunk-based stealing coincides with our synchronization-free fast-path,
and steals whole chunks in O(1) steps. Furthermore, our use of page-size chunks allows for data
migration in NUMA architectures to improve locality.

Finally, the technique of organizing data in chunks allows building dynamically sized data
structures while preserving data locality. Chunk-based data structures were previously used
in~\cite{Braginsky:2011:LLL:1946143.1946153, Gidenstam:2010:CLQ:1940234.1940266,
Hendler:2006:DNW:1160290.1160294, Sundell:2011:LAC:1989493.1989550}. SALSA extends on the idea of
using chunk-based data structures by using chunks also for efficient stealing.